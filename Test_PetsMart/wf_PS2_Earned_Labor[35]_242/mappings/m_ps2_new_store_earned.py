# Databricks notebook source
# MAGIC %run "./udf_informatica"

# COMMAND ----------


from pyspark.sql.types import *

spark.sql("use DELTA_TRAINING")
spark.sql("set spark.sql.legacy.timeParserPolicy = LEGACY")

# COMMAND ----------
%run ./MappingUtility

# COMMAND ----------
mainWorkflowId = dbutils.widgets.get("mainWorkflowId")
mainWorkflowRunId = dbutils.widgets.get("mainWorkflowRunId")
parentName = dbutils.widgets.get("parentName")
preVariableAssignment = dbutils.widgets.get("preVariableAssignment")
postVariableAssignment = dbutils.widgets.get("postVariableAssignment")
truncTargetTableOptions = dbutils.widgets.get("truncTargetTableOptions")
variablesTableName = dbutils.widgets.get("variablesTableName")

# COMMAND ----------
#Truncate Target Tables
truncateTargetTables(truncTargetTableOptions)

# COMMAND ----------
#Pre presession variable updation
updateVariable(preVariableAssignment, variablesTableName, mainWorkflowId, parentName, "m_ps2_new_store_earned")

# COMMAND ----------
fetchAndCreateVariables(parentName,"m_ps2_new_store_earned", variablesTableName, mainWorkflowId)

# COMMAND ----------
# DBTITLE 1, Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0


query_0 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  LOCATION_ID AS LOCATION_ID,
  DEPT_NAME AS DEPT_NAME,
  DRIVER_NAME AS DRIVER_NAME,
  STORE_NBR AS STORE_NBR,
  WEEK_DAY1 AS WEEK_DAY1,
  WEEK_DAY2 AS WEEK_DAY2,
  WEEK_DAY3 AS WEEK_DAY3,
  WEEK_DAY4 AS WEEK_DAY4,
  WEEK_DAY5 AS WEEK_DAY5,
  WEEK_DAY6 AS WEEK_DAY6,
  WEEK_DAY7 AS WEEK_DAY7,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  PS2_NEW_STORE_ADHOC_HRS"""

df_0 = spark.sql(query_0)

df_0.createOrReplaceTempView("Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0")

# COMMAND ----------
# DBTITLE 1, Shortcut_to_PS2_NEW_STORE_PRE1_1


query_1 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  LOCATION_ID AS LOCATION_ID,
  STORE_NBR AS STORE_NBR,
  SFT_OPEN_DT AS SFT_OPEN_DT,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  CUSTOM_AD_HOC_HRS AS CUSTOM_AD_HOC_HRS,
  GRAND_OPENING_HRS AS GRAND_OPENING_HRS
FROM
  PS2_NEW_STORE_PRE"""

df_1 = spark.sql(query_1)

df_1.createOrReplaceTempView("Shortcut_to_PS2_NEW_STORE_PRE1_1")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_2


query_2 = f"""SELECT
  Shortcut_to_PS2_NEW_STORE_PRE1_1.WEEK_DT AS WEEK_DT1,
  Shortcut_to_PS2_NEW_STORE_PRE1_1.LOCATION_ID AS LOCATION_ID1,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DT AS WEEK_DT,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.LOCATION_ID AS LOCATION_ID,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.DEPT_NAME AS DEPT_NAME,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.DRIVER_NAME AS DRIVER_NAME,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.STORE_NBR AS STORE_NBR,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DAY1 AS WEEK_DAY1,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DAY2 AS WEEK_DAY2,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DAY3 AS WEEK_DAY3,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DAY4 AS WEEK_DAY4,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DAY5 AS WEEK_DAY5,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DAY6 AS WEEK_DAY6,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DAY7 AS WEEK_DAY7,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_PS2_NEW_STORE_PRE1_1,
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0
WHERE
  Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.WEEK_DT + 6 = Shortcut_to_PS2_NEW_STORE_PRE1_1.WEEK_DT
  AND Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_0.LOCATION_ID = Shortcut_to_PS2_NEW_STORE_PRE1_1.LOCATION_ID"""

df_2 = spark.sql(query_2)

df_2.createOrReplaceTempView("SQ_Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_2")

# COMMAND ----------
# DBTITLE 1, AGG_SUM_ADHOC_HRS_3


query_3 = f"""SELECT
  WEEK_DT1 AS WEEK_DT,
  LOCATION_ID AS LOCATION_ID,
  SUM(
    WEEK_DAY1 + WEEK_DAY2 + WEEK_DAY3 + WEEK_DAY4 + WEEK_DAY5 + WEEK_DAY6 + WEEK_DAY7
  ) AS TTL_ADHOC_HRS,
  last(Monotonically_Increasing_Id) AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_PS2_NEW_STORE_ADHOC_HRS_2
GROUP BY
  WEEK_DT1,
  LOCATION_ID"""

df_3 = spark.sql(query_3)

df_3.createOrReplaceTempView("AGG_SUM_ADHOC_HRS_3")

# COMMAND ----------
# DBTITLE 1, Shortcut_to_PS2_NEW_STORE_PRE_4


query_4 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  LOCATION_ID AS LOCATION_ID,
  STORE_NBR AS STORE_NBR,
  SFT_OPEN_DT AS SFT_OPEN_DT,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  CUSTOM_AD_HOC_HRS AS CUSTOM_AD_HOC_HRS,
  GRAND_OPENING_HRS AS GRAND_OPENING_HRS
FROM
  PS2_NEW_STORE_PRE"""

df_4 = spark.sql(query_4)

df_4.createOrReplaceTempView("Shortcut_to_PS2_NEW_STORE_PRE_4")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_PS2_NEW_STORE_PRE_5


query_5 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  LOCATION_ID AS LOCATION_ID,
  STORE_NBR AS STORE_NBR,
  SFT_OPEN_DT AS SFT_OPEN_DT,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  CUSTOM_AD_HOC_HRS AS CUSTOM_AD_HOC_HRS,
  GRAND_OPENING_HRS AS GRAND_OPENING_HRS,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_PS2_NEW_STORE_PRE_4"""

df_5 = spark.sql(query_5)

df_5.createOrReplaceTempView("SQ_Shortcut_to_PS2_NEW_STORE_PRE_5")

# COMMAND ----------
# DBTITLE 1, EXP_DEFAULT_STORE_PARAMETERS_6


query_6 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  0 AS STORE_NBR,
  'Y' AS DEFAULT_STORE_FLAG,
  TRUNC(DATE_DIFF(now(), SFT_OPEN_DT, 'DD') / 7) AS v_WEEK_STORE_OPENED,
  IFF(
    PLAN_SALES_AMT > ACTUAL_SALES_AMT
    AND TRUNC(DATE_DIFF(now(), SFT_OPEN_DT, 'DD') / 7) < 4,
    PLAN_SALES_AMT,
    ACTUAL_SALES_AMT
  ) AS MATRIX_AMT,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_PS2_NEW_STORE_PRE_5"""

df_6 = spark.sql(query_6)

df_6.createOrReplaceTempView("EXP_DEFAULT_STORE_PARAMETERS_6")

# COMMAND ----------
# DBTITLE 1, LKP_PS2_NEW_STORE_MATRIX_DEFAULT_STORES_7


query_7 = f"""SELECT
  PNSM.ROW_NO AS ROW_NO,
  PNSM.START_DT AS START_DT,
  PNSM.END_DT AS END_DT,
  PNSM.DEFAULT_STORE_FLAG AS DEFAULT_STORE_FLAG,
  PNSM.STORE_NBR AS STORE_NBR,
  PNSM.L_RANGE AS L_RANGE,
  PNSM.H_RANGE AS H_RANGE,
  PNSM.TOTAL_EARNED_HOURS AS TOTAL_EARNED_HOURS,
  PNSM.UPDATE_TSTMP AS UPDATE_TSTMP,
  PNSM.LOAD_TSTMP AS LOAD_TSTMP,
  EDSP6.WEEK_DT AS WEEK_DT1,
  EDSP6.STORE_NBR AS STORE_NBR1,
  EDSP6.DEFAULT_STORE_FLAG AS DEFAULT_STORE_FLAG1,
  EDSP6.MATRIX_AMT AS MATRIX_AMT,
  EDSP6.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  EXP_DEFAULT_STORE_PARAMETERS_6 EDSP6
  LEFT JOIN PS2_NEW_STORE_MATRIX PNSM ON PNSM.STORE_NBR = EDSP6.STORE_NBR
  AND PNSM.DEFAULT_STORE_FLAG = EDSP6.DEFAULT_STORE_FLAG
  AND PNSM.START_DT < EDSP6.WEEK_DT
  AND PNSM.END_DT >= EDSP6.WEEK_DT
  AND PNSM.L_RANGE < EDSP6.MATRIX_AMT
  AND PNSM.H_RANGE >= EDSP6.MATRIX_AMT"""

df_7 = spark.sql(query_7)

df_7.createOrReplaceTempView("LKP_PS2_NEW_STORE_MATRIX_DEFAULT_STORES_7")

# COMMAND ----------
# DBTITLE 1, EX_ACTUAL_STORE_PARAMETERS_8


query_8 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  STORE_NBR AS STORE_NBR,
  'N' AS DEFAULT_STORE_FLAG,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  TRUNC(DATE_DIFF(now(), SFT_OPEN_DT, 'DD') / 7) AS v_WEEK_STORE_OPENED,
  IFF(
    PLAN_SALES_AMT > ACTUAL_SALES_AMT
    AND TRUNC(DATE_DIFF(now(), SFT_OPEN_DT, 'DD') / 7) < 4,
    PLAN_SALES_AMT,
    ACTUAL_SALES_AMT
  ) AS MATRIX_AMT,
  SFT_OPEN_DT AS SFT_OPEN_DT,
  TRUNC(DATE_DIFF(now(), SFT_OPEN_DT, 'DD') / 7) AS WEEK_STORE_OPENED,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  SQ_Shortcut_to_PS2_NEW_STORE_PRE_5"""

df_8 = spark.sql(query_8)

df_8.createOrReplaceTempView("EX_ACTUAL_STORE_PARAMETERS_8")

# COMMAND ----------
# DBTITLE 1, LKP_PS2_NEW_STORE_MATRIX_NON_DEFAULT_STORES_9


query_9 = f"""SELECT
  PNSM.ROW_NO AS ROW_NO,
  PNSM.START_DT AS START_DT,
  PNSM.END_DT AS END_DT,
  PNSM.DEFAULT_STORE_FLAG AS DEFAULT_STORE_FLAG,
  PNSM.STORE_NBR AS STORE_NBR,
  PNSM.L_RANGE AS L_RANGE,
  PNSM.H_RANGE AS H_RANGE,
  PNSM.TOTAL_EARNED_HOURS AS TOTAL_EARNED_HOURS,
  PNSM.UPDATE_TSTMP AS UPDATE_TSTMP,
  PNSM.LOAD_TSTMP AS LOAD_TSTMP,
  EASP8.WEEK_DT AS WEEK_DT,
  EASP8.STORE_NBR AS STORE_NBR1,
  EASP8.DEFAULT_STORE_FLAG AS DEFAULT_STORE_FLAG1,
  EASP8.MATRIX_AMT AS MATRIX_AMT,
  EASP8.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  EX_ACTUAL_STORE_PARAMETERS_8 EASP8
  LEFT JOIN PS2_NEW_STORE_MATRIX PNSM ON PNSM.STORE_NBR = EASP8.STORE_NBR
  AND PNSM.DEFAULT_STORE_FLAG = EASP8.DEFAULT_STORE_FLAG
  AND PNSM.START_DT < EASP8.WEEK_DT
  AND PNSM.END_DT >= EASP8.WEEK_DT
  AND PNSM.L_RANGE < EASP8.MATRIX_AMT
  AND PNSM.H_RANGE >= EASP8.MATRIX_AMT"""

df_9 = spark.sql(query_9)

df_9.createOrReplaceTempView("LKP_PS2_NEW_STORE_MATRIX_NON_DEFAULT_STORES_9")

# COMMAND ----------
# DBTITLE 1, EXP_USE_CORRECT_MATRIX_VAL_10


query_10 = f"""SELECT
  SStPNSP5.WEEK_DT AS WEEK_DT,
  SStPNSP5.LOCATION_ID AS LOCATION_ID,
  SStPNSP5.STORE_NBR AS STORE_NBR,
  EASP8.SFT_OPEN_DT AS SFT_OPEN_DT,
  EASP8.WEEK_STORE_OPENED AS WEEK_STORE_OPENED,
  SStPNSP5.PLAN_SALES_AMT AS PLAN_SALES_AMT,
  SStPNSP5.ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  SStPNSP5.GRAND_OPENING_HRS AS GRAND_OPENING_HRS,
  IFF(
    ISNULL(LPNSMNDS9.TOTAL_EARNED_HOURS),
    LPNSMDS7.TOTAL_EARNED_HOURS,
    LPNSMNDS9.TOTAL_EARNED_HOURS
  ) AS TOTAL_EARNED_HOURS,
  LPNSMNDS9.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  LKP_PS2_NEW_STORE_MATRIX_NON_DEFAULT_STORES_9 LPNSMNDS9
  INNER JOIN SQ_Shortcut_to_PS2_NEW_STORE_PRE_5 SStPNSP5 ON LPNSMNDS9.Monotonically_Increasing_Id = SStPNSP5.Monotonically_Increasing_Id
  INNER JOIN LKP_PS2_NEW_STORE_MATRIX_DEFAULT_STORES_7 LPNSMDS7 ON SStPNSP5.Monotonically_Increasing_Id = LPNSMDS7.Monotonically_Increasing_Id
  INNER JOIN EX_ACTUAL_STORE_PARAMETERS_8 EASP8 ON LPNSMDS7.Monotonically_Increasing_Id = EASP8.Monotonically_Increasing_Id"""

df_10 = spark.sql(query_10)

df_10.createOrReplaceTempView("EXP_USE_CORRECT_MATRIX_VAL_10")

# COMMAND ----------
# DBTITLE 1, JNR_ADHOC_HRS_11


query_11 = f"""SELECT
  DETAIL.WEEK_DT AS WEEK_DT,
  DETAIL.LOCATION_ID AS LOCATION_ID,
  DETAIL.STORE_NBR AS STORE_NBR,
  DETAIL.SFT_OPEN_DT AS SFT_OPEN_DT,
  DETAIL.WEEK_STORE_OPENED AS WEEK_STORE_OPENED,
  DETAIL.PLAN_SALES_AMT AS PLAN_SALES_AMT,
  DETAIL.ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  DETAIL.GRAND_OPENING_HRS AS GRAND_OPENING_HRS,
  DETAIL.TOTAL_EARNED_HOURS AS TOTAL_EARNED_HOURS,
  MASTER.WEEK_DT AS WEEK_DT1,
  MASTER.LOCATION_ID AS LOCATION_ID1,
  MASTER.TTL_ADHOC_HRS AS TTL_ADHOC_HRS,
  DETAIL.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  AGG_SUM_ADHOC_HRS_3 MASTER
  RIGHT JOIN EXP_USE_CORRECT_MATRIX_VAL_10 DETAIL ON MASTER.WEEK_DT = DETAIL.WEEK_DT
  AND MASTER.LOCATION_ID = DETAIL.LOCATION_ID"""

df_11 = spark.sql(query_11)

df_11.createOrReplaceTempView("JNR_ADHOC_HRS_11")

# COMMAND ----------
# DBTITLE 1, EXP_EARNED_TOTALS_12


query_12 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  LOCATION_ID AS LOCATION_ID,
  STORE_NBR AS STORE_NBR,
  SFT_OPEN_DT AS SFT_OPEN_DT,
  WEEK_STORE_OPENED AS WEEK_STORE_OPENED,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  IFF(ISNULL(TOTAL_EARNED_HOURS), 0, TOTAL_EARNED_HOURS) AS TOTAL_MATRIX_HOURS,
  IFF(ISNULL(TTL_ADHOC_HRS), 0, TTL_ADHOC_HRS) AS TOTAL_ADHOC_HRS,
  IFF(ISNULL(GRAND_OPENING_HRS), 0, GRAND_OPENING_HRS) AS GRAND_OPENING_HRS,
  IFF(ISNULL(TOTAL_EARNED_HOURS), 0, TOTAL_EARNED_HOURS) + IFF(ISNULL(TTL_ADHOC_HRS), 0, TTL_ADHOC_HRS) + IFF(ISNULL(GRAND_OPENING_HRS), 0, GRAND_OPENING_HRS) AS TOTAL_EARNED_HRS,
  now() AS LOAD_TSTMP,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  JNR_ADHOC_HRS_11"""

df_12 = spark.sql(query_12)

df_12.createOrReplaceTempView("EXP_EARNED_TOTALS_12")

# COMMAND ----------
# DBTITLE 1, Shortcut_to_PS2_NEW_STORE_EARNED1_13


query_13 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  LOCATION_ID AS LOCATION_ID,
  STORE_NBR AS STORE_NBR,
  SFT_OPEN_DT AS SFT_OPEN_DT,
  WEEK_STORE_OPENED AS WEEK_STORE_OPENED,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  MATRIX_HRS AS MATRIX_HRS,
  CUSTOM_AD_HOC_HRS AS CUSTOM_AD_HOC_HRS,
  GRAND_OPENING_HRS AS GRAND_OPENING_HRS,
  EARNED_HRS AS EARNED_HRS,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  PS2_NEW_STORE_EARNED"""

df_13 = spark.sql(query_13)

df_13.createOrReplaceTempView("Shortcut_to_PS2_NEW_STORE_EARNED1_13")

# COMMAND ----------
# DBTITLE 1, SQ_Shortcut_to_PS2_NEW_STORE_EARNED_14


query_14 = f"""SELECT
  WEEK_DT AS WEEK_DT,
  LOCATION_ID AS LOCATION_ID,
  STORE_NBR AS STORE_NBR,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  MATRIX_HRS AS MATRIX_HRS,
  CUSTOM_AD_HOC_HRS AS CUSTOM_AD_HOC_HRS,
  GRAND_OPENING_HRS AS GRAND_OPENING_HRS,
  EARNED_HRS AS EARNED_HRS,
  NULL AS EARNED_AMT,
  LOAD_TSTMP AS LOAD_TSTMP,
  monotonically_increasing_id() AS Monotonically_Increasing_Id
FROM
  Shortcut_to_PS2_NEW_STORE_EARNED1_13
WHERE
  Shortcut_to_PS2_NEW_STORE_EARNED1_13.WEEK_DT > CURRENT_DATE - 365"""

df_14 = spark.sql(query_14)

df_14.createOrReplaceTempView("SQ_Shortcut_to_PS2_NEW_STORE_EARNED_14")

# COMMAND ----------
# DBTITLE 1, JNR_PS2_NEW_STORE_EARNED_15


query_15 = f"""SELECT
  DETAIL.WEEK_DT AS WEEK_DT,
  DETAIL.LOCATION_ID AS LOCATION_ID,
  MASTER.WEEK_DT AS WEEK_DT1,
  MASTER.LOCATION_ID AS LOCATION_ID1,
  MASTER.STORE_NBR AS STORE_NBR,
  MASTER.SFT_OPEN_DT AS SFT_OPEN_DT,
  MASTER.WEEK_STORE_OPENED AS WEEK_STORE_OPENED,
  MASTER.PLAN_SALES_AMT AS PLAN_SALES_AMT,
  MASTER.ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  MASTER.TOTAL_MATRIX_HOURS AS TOTAL_MATRIX_HOURS,
  MASTER.TOTAL_ADHOC_HRS AS TOTAL_ADHOC_HRS,
  MASTER.GRAND_OPENING_HRS AS GRAND_OPENING_HRS,
  MASTER.TOTAL_EARNED_HRS AS TOTAL_EARNED_HRS,
  MASTER.LOAD_TSTMP AS LOAD_TSTMP,
  MASTER.Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  EXP_EARNED_TOTALS_12 MASTER
  LEFT JOIN SQ_Shortcut_to_PS2_NEW_STORE_EARNED_14 DETAIL ON MASTER.WEEK_DT = DETAIL.WEEK_DT
  AND MASTER.LOCATION_ID = DETAIL.LOCATION_ID"""

df_15 = spark.sql(query_15)

df_15.createOrReplaceTempView("JNR_PS2_NEW_STORE_EARNED_15")

# COMMAND ----------
# DBTITLE 1, FIL_EXISTING_RECORD_16


query_16 = f"""SELECT
  WEEK_DT1 AS WEEK_DT1,
  LOCATION_ID1 AS LOCATION_ID1,
  STORE_NBR AS STORE_NBR,
  SFT_OPEN_DT AS SFT_OPEN_DT,
  WEEK_STORE_OPENED AS WEEK_STORE_OPENED,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  TOTAL_MATRIX_HOURS AS TOTAL_MATRIX_HOURS,
  TOTAL_ADHOC_HRS AS TOTAL_ADHOC_HRS,
  GRAND_OPENING_HRS AS GRAND_OPENING_HRS,
  TOTAL_EARNED_HRS AS TOTAL_EARNED_HRS,
  LOAD_TSTMP AS LOAD_TSTMP,
  WEEK_DT AS target_WEEK_DT,
  Monotonically_Increasing_Id AS Monotonically_Increasing_Id
FROM
  JNR_PS2_NEW_STORE_EARNED_15
WHERE
  ISNULL(WEEK_DT)"""

df_16 = spark.sql(query_16)

df_16.createOrReplaceTempView("FIL_EXISTING_RECORD_16")

# COMMAND ----------
# DBTITLE 1, PS2_NEW_STORE_EARNED


spark.sql("""INSERT INTO
  PS2_NEW_STORE_EARNED
SELECT
  WEEK_DT1 AS WEEK_DT,
  LOCATION_ID1 AS LOCATION_ID,
  STORE_NBR AS STORE_NBR,
  SFT_OPEN_DT AS SFT_OPEN_DT,
  WEEK_STORE_OPENED AS WEEK_STORE_OPENED,
  PLAN_SALES_AMT AS PLAN_SALES_AMT,
  ACTUAL_SALES_AMT AS ACTUAL_SALES_AMT,
  TOTAL_MATRIX_HOURS AS MATRIX_HRS,
  TOTAL_ADHOC_HRS AS CUSTOM_AD_HOC_HRS,
  GRAND_OPENING_HRS AS GRAND_OPENING_HRS,
  TOTAL_EARNED_HRS AS EARNED_HRS,
  LOAD_TSTMP AS LOAD_TSTMP
FROM
  FIL_EXISTING_RECORD_16""")

# COMMAND ----------
#Post session variable updation
updateVariable(postVariableAssignment, variablesTableName, mainWorkflowId, parentName, "m_ps2_new_store_earned")

# COMMAND ----------
#Update Mapping Variables in database.
persistVariables(variablesTableName, "m_ps2_new_store_earned", mainWorkflowId, parentName)
